{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from models.model import ViTPose\n",
    "from configs.ViTPose_base_coco_256x192 import model as model_cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      ">>> Saved at: /home/cgusti/ViTPose_pytorch/vitpose_dynamic.onnx\n"
     ]
    }
   ],
   "source": [
    "CKPT_PATH = \"/home/cgusti/ViTPose_pytorch/checkpoints/vitpose-b-multi-coco.pth\"\n",
    "C, H, W = (3, 256, 192)\n",
    "\n",
    "model = ViTPose(model_cfg)\n",
    "ckpt = torch.load(CKPT_PATH)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "output_onnx = 'vitpose_dynamic.onnx'\n",
    "input_names = [\"input_0\"]\n",
    "output_names = [\"output_0\"]\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "inputs = torch.randn(1, C, H, W).to(device)\n",
    "\n",
    "dynamic_axes = {'input_0' : {0 : 'batch_size'},\n",
    "                'output_0' : {0 : 'batch_size'}}\n",
    "\n",
    "torch_out = torch.onnx.export(model, inputs, output_onnx, export_params=True, verbose=False,\n",
    "                              input_names=input_names, output_names=output_names, \n",
    "                              opset_version=11, dynamic_axes = dynamic_axes)\n",
    "print(f\">>> Saved at: {os.path.abspath(output_onnx)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Original image size: 1000 X 1500 (height X width)\n",
      ">>> Resized image size: 256 X 192 (height X width)\n",
      ">>> Scale change: 3.90625, 7.8125\n",
      ">>> Output size: (1, 17, 64, 48) ---> 0.0642 sec. elapsed [ 15.6 fps]\n",
      "\n",
      "this line is working\n",
      "this line is working2\n",
      "this line is working3\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'line'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m pid, point \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(points):\n\u001b[1;32m     51\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mthis line is working3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     img \u001b[39m=\u001b[39m draw_points_and_skeleton(img\u001b[39m.\u001b[39;49mcopy(), point, joints_dict()[\u001b[39m'\u001b[39;49m\u001b[39mcoco\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mskeleton\u001b[39;49m\u001b[39m'\u001b[39;49m], person_index\u001b[39m=\u001b[39;49mpid,\n\u001b[1;32m     53\u001b[0m                                     points_color_palette\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgist_rainbow\u001b[39;49m\u001b[39m'\u001b[39;49m, skeleton_color_palette\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mjet\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     54\u001b[0m                                     points_palette_samples\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, confidence_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m)\n\u001b[1;32m     55\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mshowing image\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/ViTPose_pytorch/utils/visualization.py:190\u001b[0m, in \u001b[0;36mdraw_points_and_skeleton\u001b[0;34m(image, points, skeleton, points_color_palette, points_palette_samples, skeleton_color_palette, skeleton_palette_samples, person_index, confidence_threshold)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_points_and_skeleton\u001b[39m(image, points, skeleton, points_color_palette\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtab20\u001b[39m\u001b[39m'\u001b[39m, points_palette_samples\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m    160\u001b[0m                              skeleton_color_palette\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSet2\u001b[39m\u001b[39m'\u001b[39m, skeleton_palette_samples\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, person_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    161\u001b[0m                              confidence_threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m    162\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    Draws `points` and `skeleton` on `image`.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     image \u001b[39m=\u001b[39m draw_skeleton(image, points, skeleton, color_palette\u001b[39m=\u001b[39;49mskeleton_color_palette,\n\u001b[1;32m    191\u001b[0m                           palette_samples\u001b[39m=\u001b[39;49mskeleton_palette_samples, person_index\u001b[39m=\u001b[39;49mperson_index,\n\u001b[1;32m    192\u001b[0m                           confidence_threshold\u001b[39m=\u001b[39;49mconfidence_threshold)\n\u001b[1;32m    193\u001b[0m     image \u001b[39m=\u001b[39m draw_points(image, points, color_palette\u001b[39m=\u001b[39mpoints_color_palette, palette_samples\u001b[39m=\u001b[39mpoints_palette_samples,\n\u001b[1;32m    194\u001b[0m                         confidence_threshold\u001b[39m=\u001b[39mconfidence_threshold)\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/ViTPose_pytorch/utils/visualization.py:151\u001b[0m, in \u001b[0;36mdraw_skeleton\u001b[0;34m(image, points, skeleton, color_palette, palette_samples, person_index, confidence_threshold)\u001b[0m\n\u001b[1;32m    149\u001b[0m     pt1, pt2 \u001b[39m=\u001b[39m points[joint]\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m pt1[\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m confidence_threshold \u001b[39mand\u001b[39;00m pt2[\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m confidence_threshold:\n\u001b[0;32m--> 151\u001b[0m         image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mline(\n\u001b[1;32m    152\u001b[0m             image, (\u001b[39mint\u001b[39;49m(pt1[\u001b[39m1\u001b[39;49m]), \u001b[39mint\u001b[39;49m(pt1[\u001b[39m0\u001b[39;49m])), (\u001b[39mint\u001b[39;49m(pt2[\u001b[39m1\u001b[39;49m]), \u001b[39mint\u001b[39;49m(pt2[\u001b[39m0\u001b[39;49m])),\n\u001b[1;32m    153\u001b[0m             \u001b[39mtuple\u001b[39;49m(colors[person_index \u001b[39m%\u001b[39;49m \u001b[39mlen\u001b[39;49m(colors)]), \u001b[39m2\u001b[39;49m\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'line'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = \"/home/cgusti/ViTPose_pytorch/examples/yoga_pose_3.jpeg\"\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from utils.visualization import draw_points_and_skeleton, joints_dict\n",
    "from utils.dist_util import get_dist_info, init_dist\n",
    "from utils.top_down_eval import keypoints_from_heatmaps\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(output_onnx)\n",
    "\n",
    "# Prepare input data\n",
    "img = Image.open(IMG_PATH)\n",
    "\n",
    "org_w, org_h = img.size\n",
    "print(f\">>> Original image size: {org_h} X {org_w} (height X width)\")\n",
    "print(f\">>> Resized image size: {H} X {W} (height X width)\")\n",
    "print(f\">>> Scale change: {org_h/H}, {org_w/W}\")\n",
    "img_tensor = transforms.Compose (\n",
    "    [transforms.Resize((H, W)),\n",
    "        transforms.ToTensor()]\n",
    ")(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Feed to model\n",
    "tic = time()\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_tensor)}\n",
    "heatmaps = ort_session.run(None, ort_inputs)[0]\n",
    "# heatmaps = vit_pose(img_tensor).detach().cpu().numpy() # N, 17, h/4, w/4\n",
    "elapsed_time = time()-tic\n",
    "print(f\">>> Output size: {heatmaps.shape} ---> {elapsed_time:.4f} sec. elapsed [{elapsed_time**-1: .1f} fps]\\n\")    \n",
    "\n",
    "print('this line is working')\n",
    "# points = heatmap2coords(heatmaps=heatmaps, original_resolution=(org_h, org_w))\n",
    "points, prob = keypoints_from_heatmaps(heatmaps=heatmaps, center=np.array([[org_w//2, org_h//2]]), scale=np.array([[org_w, org_h]]),\n",
    "                                        unbiased=True, use_udp=True)\n",
    "points = np.concatenate([points[:, :, ::-1], prob], axis=2)\n",
    "print('this line is working2')\n",
    "# Visualization \n",
    "for pid, point in enumerate(points):\n",
    "    print('this line is working3')\n",
    "    img = draw_points_and_skeleton(img.copy(), point, joints_dict()['coco']['skeleton'], person_index=pid,\n",
    "                                    points_color_palette='gist_rainbow', skeleton_color_palette='jet',\n",
    "                                    points_palette_samples=10, confidence_threshold=0.4)\n",
    "    plt.figure(figsize=(5,10))\n",
    "    print('showing image')\n",
    "    plt.imshow(img)\n",
    "    print('showing image is successful')\n",
    "    plt.title(\"Result\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
